
### CFN based on mlp
1. Make sure you have `torch` and `transformers`

2. Install cfn

```bash
cd your_path/VLA-tts
pip install -e .
```

3. Useage

```python
    from cfn.cfn_net import CFNWrapper
    import torch

    ### load model
    model = CFNWrapper(
        state_dim=state_dim,
        action_dim=cfn_action_steps * action_dim,
        language_model_name="bert-base-uncased",
        embed_dim=128,
        cfn_output_dim=20,
    ).to(device)

    weight_path = 'your_path/model_epoch1.pt'
    print(f"ğŸ” åŠ è½½æ¨¡å‹æƒé‡: {weight_path}")
    model.load_state_dict(torch.load(weight_path, map_location=device))
    model.eval()

    with torch.no_grad():
        ### Calculate norm
        state = batch['observation.state'].to(device)
        action = action[:, :cfn_action_steps, :].to(device)
        task = batch['task']
        batch_size = action.shape[0]
        action_flat = action.reshape(batch_size, -1)

        cfn_output = model(state, action_flat, task)

        ### æ‰¾æœ€å°normå¯¹åº”çš„action
        
        norm = cfn_output.norm(dim=1)
        # 1. æ‰¾åˆ°æœ€å°å€¼
        min_val = torch.min(norm)
        # 2. æ‰¾å‡ºæ‰€æœ‰ç­‰äºæœ€å°å€¼çš„ç´¢å¼•
        indices = torch.nonzero(norm == min_val).squeeze()
        # 3. ä»ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªç´¢å¼•
        if indices.ndim == 0:
            # åªæœ‰ä¸€ä¸ªæœ€å°å€¼çš„æƒ…å†µ
            selected_index = indices.item()
        else:
            selected_index = indices[torch.randint(0, len(indices), (1,))].item()    

        outputs_action = batch['action'][[selected_index], ...]

```

